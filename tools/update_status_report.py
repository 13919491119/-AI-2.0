#!/usr/bin/env python3
"""
å®šæ—¶ç”Ÿæˆâ€œå­¦ä¹ /å¤ç›˜/é¢„æµ‹/å‡çº§â€è¿è¡ŒçŠ¶æ€æŠ¥å‘Šï¼Œè¦†ç›– reports/ssq_status_YYYYMMDD.mdã€‚
æ¥æºï¼šsupervisor çŠ¶æ€ã€å…³é”®æ—¥å¿—ã€æ±‡æ€»æŒ‡æ ‡ã€‚
"""
import subprocess, datetime, json, os, re, glob
from typing import Tuple
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except Exception:  # æç«¯æƒ…å†µä¸‹ä¸å¯ç”¨æ—¶å›é€€
    ZoneInfo = None  # type: ignore

ROOT = "/workspaces/-AI-2.0"
CONF = os.path.join(ROOT, "supervisord.conf")
LOG_DIR = os.path.join(ROOT, "logs", "supervisor")
REPORTS = os.path.join(ROOT, "reports")
PERSON_LOG = os.path.join(ROOT, "xuanji_person_predict.log")

def parse_operation_report(md_text: str) -> dict:
    """
    è§£æè¿è¥å‘¨æœŸæŠ¥å‘Š Markdownï¼Œæå–å…³é”®æŒ‡æ ‡ç”Ÿæˆæ‘˜è¦ JSONã€‚
    é‡‡ç”¨å®½æ¾æ­£åˆ™ï¼Œç¼ºå¤±å­—æ®µä»¥ None è¿”å›ã€‚
    """
    def find(pattern: str, flags=0, cast=str):
        m = re.search(pattern, md_text, flags)
        if not m:
            return None
        val = m.group(1).strip()
        if cast is int:
            try:
                return int(re.sub(r"[^0-9]", "", val) or "0")
            except Exception:
                return None
        if cast is float:
            try:
                return float(re.sub(r"[^0-9.]+", "", val))
            except Exception:
                return None
        return val

    report_time = find(r"æŠ¥å‘Šç”Ÿæˆæ—¶é—´\*\*ï¼š\s*([0-9\-:]+)")
    data = {
        "report_time": report_time,
        "core": {
            "ç´¯è®¡å­¦ä¹ å‘¨æœŸ": find(r"ç´¯è®¡å­¦ä¹ å‘¨æœŸ\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "çŸ¥è¯†å¢é•¿é‡": find(r"çŸ¥è¯†å¢é•¿é‡\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "ç³»ç»Ÿä¼˜åŒ–è¿›åº¦": find(r"ç³»ç»Ÿä¼˜åŒ–è¿›åº¦\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "è¿è¡Œå‘¨æœŸ": find(r"è¿è¡Œå‘¨æœŸ\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "ç³»ç»Ÿè‡ªä¸»å‡çº§": find(r"ç³»ç»Ÿè‡ªä¸»å‡çº§\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "æ€§èƒ½æå‡å€æ•°": find(r"æ€§èƒ½æå‡å€æ•°\*\*ï¼š\s*([0-9\.]+)", cast=float),
        },
        "ssq": {
            "åŒè‰²çƒå­¦ä¹ å‘¨æœŸ": find(r"åŒè‰²çƒå­¦ä¹ å‘¨æœŸ\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "å®Œå…¨åŒ¹é…æ¬¡æ•°": find(r"å®Œå…¨åŒ¹é…æ¬¡æ•°\*\*ï¼š\s*([0-9,\.]+)", cast=int),
            "åŒ¹é…å†å²": find(r"åŒ¹é…å†å²\*\*ï¼š\s*([^\n]+)"),
            "å¹³å‡å°è¯•æ¬¡æ•°": find(r"å¹³å‡å°è¯•æ¬¡æ•°\*\*ï¼š\s*([^\n]+)"),
            "æ¨¡å‹æƒé‡åˆ†å¸ƒ": find(r"æ¨¡å‹æƒé‡åˆ†å¸ƒ\*\*ï¼š\s*([^\n]+)"),
            "ç´¯è®¡æ•°æ®æºå­¦ä¹ è½®æ¬¡": find(r"ç´¯è®¡æ•°æ®æºå­¦ä¹ è½®æ¬¡\*\*ï¼š\s*([0-9,\.]+)", cast=int),
        },
    }
    return data

def sh(cmd: str) -> str:
    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    return p.stdout.strip()

def get_status():
    out = sh(f"supervisorctl -c {CONF} status")
    lines = [l for l in out.splitlines() if l.strip()]
    return "\n".join(lines)

def parse_status(status_text: str):
    """Parse supervisorctl status text into structured entries.
    Best-effort: extract first two whitespace-separated tokens as name/status.
    """
    entries = []
    for raw in status_text.splitlines():
        line = raw.strip()
        if not line:
            continue
        parts = re.split(r"\s+", line)
        if len(parts) >= 2:
            name, status = parts[0], parts[1]
            # å°è¯•ä» raw ä¸­è§£æ pid ä¸ uptime
            pid = None
            uptime = None
            m_pid = re.search(r"pid\s+(\d+)", line)
            if m_pid:
                try:
                    pid = int(m_pid.group(1))
                except Exception:
                    pid = None
            m_up = re.search(r"uptime\s+([0-9:]+)", line)
            if m_up:
                uptime = m_up.group(1)
            entries.append({"name": name, "status": status, "pid": pid, "uptime": uptime, "raw": line})
        else:
            entries.append({"name": line, "status": "UNKNOWN", "raw": line})
    return entries

def tail_log(name: str, n: int = 30) -> str:
    outp = os.path.join(LOG_DIR, f"{name}.out.log")
    errp = os.path.join(LOG_DIR, f"{name}.err.log")
    def tail(p):
        if not os.path.exists(p):
            return "(æ— )"
        return sh(f"tail -n {n} {p} || true") or "(ç©º)"
    return f"### {name} OUT\n{tail(outp)}\n\n### {name} ERR\n{tail(errp)}"

def load_metrics():
    p = os.path.join(REPORTS, "ssq_cycle_summary.json")
    if not os.path.exists(p):
        return {}
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)

def load_person_metrics(max_bytes: int = 20*1024*1024, last_n: int = 20):
    """Parse historical person task log for lightweight metrics.
    - If log too large (> max_bytes), only parse the last 'last_n' lines for preview,
      but still try to count totals by streaming.
    """
    if not os.path.exists(PERSON_LOG):
        return {}
    try:
        size = os.path.getsize(PERSON_LOG)
        mtime = datetime.datetime.fromtimestamp(os.path.getmtime(PERSON_LOG)).strftime("%Y-%m-%d %H:%M:%S")
        total = 0
        per = {}
        matches_true = {}
        last_lines = []
        # Stream parse for counts (line by line)
        with open(PERSON_LOG, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                # Example: "å¤ç›˜å‘¨æœŸ70ï¼šçˆ±å› æ–¯å¦ ï¼Œé¢„æµ‹ï¼š...ï¼Œäº‹å®ï¼š...ï¼Œå»åˆï¼šFalse"
                if "å¤ç›˜å‘¨æœŸ" in line:
                    total += 1
                    # extract name between full-width punctuation after colon up to first comma-like
                    # Tolerate spaces and potential variants
                    m = re.search(r"å¤ç›˜å‘¨æœŸ\d+ï¼š\s*([^ï¼Œ,]+)", line)
                    name = m.group(1).strip() if m else "æœªçŸ¥"
                    per[name] = per.get(name, 0) + 1
                    # match flag
                    m2 = re.search(r"å»åˆï¼š\s*(True|False)", line)
                    if m2 and m2.group(1) == "True":
                        matches_true[name] = matches_true.get(name, 0) + 1
        # collect tail preview
        try:
            tail_text = sh(f"tail -n {last_n} '{PERSON_LOG}' || true")
        except Exception:
            tail_text = ""
        # compose metrics
        persons = {}
        for name, cnt in per.items():
            persons[name] = {"count": cnt, "matches_true": matches_true.get(name, 0)}
        return {
            "log_mtime": mtime,
            "size": size,
            "total_records": total,
            "per_person": persons,
            "last_tail": tail_text,
        }
    except Exception:
        return {}

def _now_times() -> Tuple[str, str, str]:
    """Return (utc_str, local_str, local_tz_name).
    - utc_str in format: YYYY-mm-dd HH:MM:SS UTC +0000
    - local_str in format: YYYY-mm-dd HH:MM:SS CST +0800 (Asia/Shanghai)
    """
    # UTC
    utc_dt = datetime.datetime.now(datetime.timezone.utc)
    utc_str = utc_dt.strftime("%Y-%m-%d %H:%M:%S %Z %z")
    # Local (Asia/Shanghai)
    tz_name = "Asia/Shanghai"
    try:
        if ZoneInfo is not None:
            local_dt = utc_dt.astimezone(ZoneInfo(tz_name))
        else:
            raise RuntimeError("ZoneInfo unavailable")
    except Exception:
        # æ—  ZoneInfo æ—¶å›ºå®š +08:00 åç§»ï¼ˆä¸­å›½ä¸å®è¡Œå¤ä»¤æ—¶ï¼‰
        local_dt = utc_dt.astimezone(datetime.timezone(datetime.timedelta(hours=8)))
        tz_name = "Asia/Shanghai"
    local_str = local_dt.strftime("%Y-%m-%d %H:%M:%S %Z %z")
    return utc_str, local_str, tz_name

def write_report(md_path: str, status_text: str, logs: str, metrics: dict, person_metrics: dict):
    ts_utc, ts_local, tz_local = _now_times()
    lines = [
        "# åŒè‰²çƒå­¦ä¹ /å¤ç›˜/é¢„æµ‹/å‡çº§ è‡ªå¾ªç¯çŠ¶æ€æŠ¥å‘Šï¼ˆå®æ—¶ï¼‰",
        f"\n**ç”Ÿæˆæ—¶é—´ï¼ˆUTCï¼‰**ï¼š{ts_utc}\n\n**åŒ—äº¬æ—¶é—´ï¼ˆ{tz_local}ï¼‰**ï¼š{ts_local}\n",
        "## 1. åå°è¿›ç¨‹çŠ¶æ€ï¼ˆsupervisor æ‰˜ç®¡ï¼‰",
        f"\n````\n{status_text}\n````\n",
        "## 2. å…³é”®æ—¥å¿—å¿«ç…§ï¼ˆæœ€è¿‘ï¼‰",
        logs,
    ]
    if metrics:
        lines += [
            "\n## 3. ç´¯ç§¯è¯„ä¼°æŒ‡æ ‡ï¼ˆreports/ssq_cycle_summary.jsonï¼‰",
            f"\n- ç»Ÿè®¡æ—¶é—´ï¼š{metrics.get('timestamp','-')}\n- æ€»é¢„æµ‹æ¬¡æ•°ï¼š{metrics.get('total_predictions','-')}\n- å‘½ä¸­æ¬¡æ•°ï¼š{metrics.get('total_matches','-')}\n",
            "- æ¨¡å‹åˆ†å¸ƒï¼š",
        ]
        by = metrics.get("by_model", {})
        for k in ("liuyao","liuren","qimen","ai"):
            if k in by:
                lines.append(f"  - {k}ï¼š{by[k].get('count','-')}ï¼ˆå‘½ä¸­ {by[k].get('matches','-')}ï¼‰")
    # person metrics
    if person_metrics:
        lines += [
            "\n## 4. å†å²äººç‰©ä»»åŠ¡æŒ‡æ ‡ï¼ˆxuanji_person_predict.logï¼‰",
            f"\n- æœ€è¿‘æ›´æ–°æ—¶é—´ï¼š{person_metrics.get('log_mtime','-')}\n- æ—¥å¿—å¤§å°ï¼š{person_metrics.get('size','-')} å­—èŠ‚\n- æ€»å¤ç›˜è®°å½•ï¼š{person_metrics.get('total_records','-')}\n",
            "- äººç‰©ç»Ÿè®¡ï¼š",
        ]
        for name, row in sorted(person_metrics.get("per_person", {}).items(), key=lambda kv: kv[0]):
            lines.append(f"  - {name}ï¼š{row.get('count','-')}ï¼ˆå»åˆ=True {row.get('matches_true',0)}ï¼‰")
        tail = person_metrics.get("last_tail", "")
        if tail:
            lines += ["\n- æœ€è¿‘ç‰‡æ®µï¼š\n", f"````\n{tail}\n````\n"]
        # Top3 summary by count
        per = person_metrics.get("per_person", {})
        if per:
            top3 = sorted(per.items(), key=lambda kv: kv[1].get("count", 0), reverse=True)[:3]
            if top3:
                lines += ["\n- Top3 äººç‰©ï¼ˆæŒ‰å¤ç›˜è®°å½•æ•°ï¼‰ï¼š"]
                for name, row in top3:
                    lines.append(f"  - {name}: {row.get('count',0)}ï¼ˆå»åˆ=True {row.get('matches_true',0)}ï¼‰")
    lines += [
    "\n## 5. ç»“è®ºä¸å»ºè®®",
        "\n- ç»“è®ºï¼šæœåŠ¡ RUNNINGï¼›å­¦ä¹ /é¢„æµ‹/ä¼˜åŒ–å¾ªç¯æ— è‡´å‘½æŠ¥é”™ã€‚å¦‚æ—¥å¿—å­˜åœ¨é˜¶æ®µæ€§ 404ï¼ˆ/discoverï¼‰ï¼Œä¸ºéå…¬å¼€æ¥å£ã€‚",
        "- å»ºè®®ï¼šå¦‚éœ€å¯¹å¤–å…¬å¼€ï¼Œè¯·é…ç½® ngrok/FRP æˆ– Nginx + è¯ä¹¦ï¼›å»ºè®®å¯ç”¨æ—¥å¿—è½®è½¬æˆ–æ¥å…¥ Prometheus/Grafanaã€‚",
        "\nâ€” æœ¬æŠ¥å‘Šç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå¹¶è¦†ç›–æ›´æ–°",
    ]
    content = "\n".join(lines)
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(content)
    # åŒæ­¥ç”Ÿæˆé™æ€ HTML ç‰ˆæœ¬ï¼Œä¾¿äºæ— éœ€æ¡†æ¶çš„å‰ç«¯å±•ç¤º
    html_out = os.path.join(ROOT, "static", "status_report.html")
    os.makedirs(os.path.dirname(html_out), exist_ok=True)
    # æ„é€ å¸¦é”šç‚¹çš„é™æ€ HTMLï¼Œä¾¿äºå¿«é€Ÿè·³è½¬åˆ°å„ä»»åŠ¡æ—¥å¿—ä¸äººç‰©æŒ‡æ ‡
    # ä¸ºé¿å…å¤æ‚è§£æ Markdownï¼Œè¿™é‡Œé¢å¤–ç›´æ¥æ³¨å…¥å…³é”®æ®µè½çš„é”šç‚¹ç‰ˆæœ¬ã€‚
    # å†é™„ä¸ŠåŸ Markdown çš„çº¯å±•ç¤ºä½œä¸ºå‚è€ƒã€‚
    def esc(t: str) -> str:
        return t.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    # åˆ†åˆ«æŠ“å–å„æ—¥å¿—æœ€æ–°å†…å®¹
    log_sections = {
        "autonomous": tail_log("xuanji_autonomous"),
        "predict": tail_log("xuanji_predict"),
        "optimize": tail_log("xuanji_optimize"),
        "person": tail_log("xuanji_person"),
        "api": tail_log("xuanji_api"),
    }
    # Render person metrics block
    def render_person_metrics(pm: dict) -> str:
        if not pm:
            return "(æ— äººç‰©æŒ‡æ ‡)"
        lines = [
            f"æœ€è¿‘æ›´æ–°æ—¶é—´ï¼š{pm.get('log_mtime','-')}",
            f"æ—¥å¿—å¤§å°ï¼š{pm.get('size','-')} å­—èŠ‚",
            f"æ€»å¤ç›˜è®°å½•ï¼š{pm.get('total_records','-')}",
            "äººç‰©ç»Ÿè®¡ï¼š",
        ]
        for name, row in sorted(pm.get('per_person', {}).items(), key=lambda kv: kv[0]):
            lines.append(f"  - {name}ï¼š{row.get('count','-')}ï¼ˆå»åˆ=True {row.get('matches_true',0)}ï¼‰")
        tail = pm.get('last_tail', '')
        if tail:
            lines.append("\næœ€è¿‘ç‰‡æ®µï¼š\n" + tail)
        return "\n".join(lines)

    # simple bar chart for person metrics
    def render_person_barchart(pm: dict) -> str:
        if not pm or not pm.get('per_person'):
            return "<div class='note'>(æ— æ•°æ®)</div>"
        per = pm.get('per_person', {})
        items = sorted(per.items(), key=lambda kv: kv[1].get('count', 0), reverse=True)
        # Only show top 10 for readability
        items = items[:10]
        maxv = max((row.get('count', 0) for _, row in items), default=1) or 1
        bars = []
        for name, row in items:
            cnt = row.get('count', 0)
            mt = row.get('matches_true', 0)
            width = int(100 * cnt / maxv)
            bars.append(f"""
            <div style='display:flex; align-items:center; gap:8px; margin:6px 0;'>
                <div style='width:140px;'>{name}</div>
                <div style='flex:1; background:#e5e7eb; border-radius:6px; overflow:hidden;'>
                    <div style='width:{width}%; background:#60a5fa; height:16px;'></div>
                </div>
                <div style='width:120px; text-align:right;'><code>{cnt}</code> / <span title='å»åˆ=True æ¬¡æ•°'><code>{mt}</code></span></div>
            </div>
            """)
        return "\n".join(bars)
    html = f"""
    <!DOCTYPE html>
    <html lang='zh-CN'>
    <head>
        <meta charset='utf-8'>
        <title>æœ€æ–°è¿è¡ŒçŠ¶æ€æŠ¥å‘Š</title>
        <link rel='stylesheet' href='/static/report.css'>
        <style>
            body {{ font-family: system-ui, -apple-system, Segoe UI, Roboto, 'Helvetica Neue', Arial, 'Noto Sans', 'Liberation Sans', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif; }}
            pre {{ white-space: pre-wrap; word-break: break-word; background:#0b1020; color:#e6edf3; padding:12px; border-radius:8px; }}
            .container {{ max-width: 980px; margin: 32px auto; padding: 0 16px; }}
            .nav {{ display:flex; flex-wrap:wrap; gap:12px; margin: 12px 0 20px; }}
            .nav a {{ text-decoration:none; background:#eef2ff; color:#1d4ed8; padding:6px 10px; border-radius:6px; font-size:14px; }}
            h2 {{ margin-top: 28px; }}
            .meta {{ color:#6b7280; font-size: 14px; }}
        </style>
    </head>
    <body>
        <div class='container'>
            <h1>ğŸ›°ï¸ æœ€æ–°è¿è¡ŒçŠ¶æ€æŠ¥å‘Š</h1>
            <div class='meta'>æ–‡ä»¶ï¼š{os.path.basename(md_path)}ï½œç”Ÿæˆæ—¶é—´ï¼ˆUTCï¼‰ï¼š{ts_utc}ï½œåŒ—äº¬æ—¶é—´ï¼ˆ{tz_local}ï¼‰ï¼š{ts_local}</div>
            <div class='nav'>
                <a href="#sec-status">è¿›ç¨‹çŠ¶æ€</a>
                <a href="#sec-autonomous">å­¦ä¹ /å¤ç›˜æ—¥å¿—</a>
                <a href="#sec-predict">åŒè‰²çƒé¢„æµ‹æ—¥å¿—</a>
                <a href="#sec-optimize">ä¼˜åŒ–å¾ªç¯æ—¥å¿—</a>
                <a href="#sec-person">å†å²äººç‰©ä»»åŠ¡æ—¥å¿—</a>
                <a href="#sec-api">API æ—¥å¿—</a>
                <a href="#sec-person-metrics">äººç‰©ä»»åŠ¡æŒ‡æ ‡</a>
                <a href="#sec-full">åŸå§‹å…¨æ–‡</a>
            </div>

            <h2 id='sec-status'>1ï¼‰è¿›ç¨‹çŠ¶æ€</h2>
            <pre>{esc(status_text)}</pre>

            <h2 id='sec-autonomous'>2ï¼‰å­¦ä¹ /å¤ç›˜æ—¥å¿—</h2>
            <pre>{esc(log_sections['autonomous'])}</pre>

            <h2 id='sec-predict'>3ï¼‰åŒè‰²çƒé¢„æµ‹æ—¥å¿—</h2>
            <pre>{esc(log_sections['predict'])}</pre>

            <h2 id='sec-optimize'>4ï¼‰ä¼˜åŒ–å¾ªç¯æ—¥å¿—</h2>
            <pre>{esc(log_sections['optimize'])}</pre>

            <h2 id='sec-person'>5ï¼‰å†å²äººç‰©ä»»åŠ¡æ—¥å¿—</h2>
            <pre>{esc(log_sections['person'])}</pre>

            <h2 id='sec-api'>6ï¼‰API æ—¥å¿—</h2>
            <pre>{esc(log_sections['api'])}</pre>

            <h2 id='sec-person-metrics'>7ï¼‰äººç‰©ä»»åŠ¡æŒ‡æ ‡</h2>
            <pre>{esc(render_person_metrics(person_metrics))}</pre>

            <h2>8ï¼‰äººç‰©ä»»åŠ¡å›¾è¡¨ï¼ˆTop10ï¼‰</h2>
            <div style='border:1px solid #e5e7eb; border-radius:8px; padding:12px;'>
                {render_person_barchart(person_metrics)}
            </div>

            <h2 id='sec-full'>é™„å½•ï¼šåŸå§‹ Markdown å…¨æ–‡</h2>
            <pre>{esc(content)}</pre>
        </div>
    </body>
    </html>
    """
    with open(html_out, "w", encoding="utf-8") as f:
        f.write(html)
    # ç”Ÿæˆæœ€æ–°è¿è¥å‘¨æœŸæŠ¥å‘Šçš„é™æ€ HTML å’Œæ‘˜è¦ JSONï¼ˆè‹¥å­˜åœ¨ï¼‰
    try:
        op_paths = sorted(glob.glob(os.path.join(REPORTS, 'operation_report_*.md')))
        if op_paths:
            op_md = op_paths[-1]
            with open(op_md, 'r', encoding='utf-8') as rf:
                op_content = rf.read()
            op_html = os.path.join(ROOT, 'static', 'operation_report.html')
            op_doc = f"""
            <!DOCTYPE html>
            <html lang='zh-CN'>
            <head>
                <meta charset='utf-8'>
                <title>æœ€æ–°è¿è¥å‘¨æœŸæŠ¥å‘Š</title>
                <link rel='stylesheet' href='/static/report.css'>
                <style>
                    pre {{ white-space: pre-wrap; word-break: break-word; }}
                    .container {{ max-width: 980px; margin: 32px auto; padding: 0 16px; }}
                    .meta {{ color:#6b7280; font-size: 14px; }}
                </style>
            </head>
            <body>
                <div class='container'>
                    <h1>ğŸ“ˆ æœ€æ–°è¿è¥å‘¨æœŸæŠ¥å‘Š</h1>
                    <div class='meta'>æ–‡ä»¶ï¼š{os.path.basename(op_md)}ï½œç”Ÿæˆæ—¶é—´ï¼ˆUTCï¼‰ï¼š{ts_utc}ï½œåŒ—äº¬æ—¶é—´ï¼ˆ{tz_local}ï¼‰ï¼š{ts_local}</div>
                    <pre>{esc(op_content)}</pre>
                </div>
            </body>
            </html>
            """
            with open(op_html, 'w', encoding='utf-8') as of:
                of.write(op_doc)
            # åŒæ­¥ç”Ÿæˆæ‘˜è¦ JSON
            try:
                summary = parse_operation_report(op_content)
                summary.update({
                    "file": os.path.basename(op_md),
                    "generated_utc": ts_utc,
                    "generated_local": ts_local,
                })
                with open(os.path.join(ROOT, 'static', 'operation_summary.json'), 'w', encoding='utf-8') as sf:
                    json.dump(summary, sf, ensure_ascii=False, indent=2)
            except Exception:
                pass
    except Exception:
        pass
    # é¢å¤–ç”Ÿæˆ JSON å¿«ç…§ï¼Œä¾¿äºç¨‹åºæ¶ˆè´¹
    try:
        json_out = os.path.join(ROOT, "static", "status.json")
        # è§£æå¿ƒè·³ï¼šå…³æ³¨ xuanji_predict ä¸ xuanji_person
        sup_entries = parse_status(status_text)
        hb = {}
        # æ—¥å¿—æ—¶é—´ä¸å°¾éƒ¨
        def _log_meta(prog: str):
            outp = os.path.join(LOG_DIR, f"{prog}.out.log")
            ts = None
            tail = None
            if os.path.exists(outp):
                try:
                    ts = datetime.datetime.fromtimestamp(os.path.getmtime(outp)).strftime("%Y-%m-%d %H:%M:%S")
                except Exception:
                    ts = None
                try:
                    tail = sh(f"tail -n 10 {outp} || true")
                except Exception:
                    tail = None
            return ts, tail
        for prog in ("xuanji_predict", "xuanji_person"):
            row = next((e for e in sup_entries if e.get("name") == prog), None)
            last_ts, last_tail = _log_meta(prog)
            if row:
                hb[prog] = {
                    "status": row.get("status"),
                    "pid": row.get("pid"),
                    "uptime": row.get("uptime"),
                    "last_log_time": last_ts,
                    "last_tail": last_tail,
                }
            else:
                hb[prog] = {
                    "status": "UNKNOWN",
                    "pid": None,
                    "uptime": None,
                    "last_log_time": last_ts,
                    "last_tail": last_tail,
                }
        snapshot = {
            # å…¼å®¹ï¼šæ²¿ç”¨ timestamp ä½†æ˜¾å¼ä½¿ç”¨ UTC
            "timestamp": ts_utc,
            "local_time": ts_local,
            "timezone": tz_local,
            "supervisor": sup_entries,
            "heartbeats": hb,
            "metrics": metrics or {},
            "person": {
                "log_mtime": person_metrics.get("log_mtime") if person_metrics else None,
                "size": person_metrics.get("size") if person_metrics else None,
                "total_records": person_metrics.get("total_records") if person_metrics else None,
                "per_person": person_metrics.get("per_person") if person_metrics else None,
            },
        }
        with open(json_out, "w", encoding="utf-8") as jf:
            json.dump(snapshot, jf, ensure_ascii=False, indent=2)
    except Exception:
        pass

    # â€”â€”è‡ªæ„ˆæ£€æµ‹ä¸è‡ªåŠ¨å›é€€â€”â€”
    try:
        # 1. æ£€æŸ¥æ‰€æœ‰å…³é”®ä»»åŠ¡çŠ¶æ€
        sup_entries = snapshot.get('supervisor', [])
        unhealthy = []
        for e in sup_entries:
            if e.get('status') not in ('RUNNING', 'STARTING'):
                unhealthy.append(e)
        # 2. æ£€æŸ¥å¥åº·è¯„åˆ†
        health_score = None
        op_sum_path = os.path.join(ROOT, 'static', 'operation_summary.json')
        if os.path.exists(op_sum_path):
            with open(op_sum_path, 'r', encoding='utf-8') as f:
                op_sum = json.load(f)
            health_score = op_sum.get('core', {}).get('å¥åº·è¯„åˆ†')
        # ç³»ç»Ÿè‡ªè®¾å¥åº·è¯„åˆ†ä¸‹é™
        HEALTH_MIN = 60
        # 3. è‹¥å‘ç°å¼‚å¸¸åˆ™è‡ªåŠ¨é‡å¯å¹¶å›é€€æƒé‡
        if unhealthy or (health_score is not None and health_score < HEALTH_MIN):
            # è®°å½•è‡ªæ„ˆåŠ¨ä½œ
            action_log = os.path.join(ROOT, 'static', 'self_heal_log.txt')
            with open(action_log, 'a', encoding='utf-8') as af:
                af.write(f"[{__import__('datetime').datetime.now()}] è‡ªæ„ˆè§¦å‘: å¼‚å¸¸ä»»åŠ¡={unhealthy}, å¥åº·è¯„åˆ†={health_score}\n")
            # è‡ªåŠ¨é‡å¯å¼‚å¸¸ä»»åŠ¡
            for e in unhealthy:
                name = e.get('name')
                if name:
                    os.system(f".venv/bin/supervisorctl -c supervisord.conf restart {name}")
            # è‡ªåŠ¨å›é€€æƒé‡æ–‡ä»¶ï¼ˆå¦‚æœ‰å†å²ï¼‰
            hist_path = os.path.join(ROOT, 'reports', 'ssq_weights_history.jsonl')
            wfile = os.path.join(ROOT, 'ssq_strategy_weights.json')
            if os.path.exists(hist_path):
                try:
                    with open(hist_path, 'r', encoding='utf-8') as hf:
                        lines = [l for l in hf if l.strip()]
                    if len(lines) >= 2:
                        # å›é€€åˆ°å€’æ•°ç¬¬äºŒæ¡
                        last_good = json.loads(lines[-2])
                        with open(wfile, 'w', encoding='utf-8') as wf:
                            json.dump(last_good, wf, ensure_ascii=False, indent=2)
                        with open(action_log, 'a', encoding='utf-8') as af:
                            af.write(f"[{__import__('datetime').datetime.now()}] æƒé‡è‡ªåŠ¨å›é€€åˆ°å†å²ç‰ˆæœ¬\n")
                except Exception:
                    pass
    except Exception:
        pass
    # ç”Ÿæˆè°ƒä¼˜æ‘˜è¦ tuning_summary.jsonï¼ˆè‹¥å­˜åœ¨æƒé‡æ–‡ä»¶ï¼‰
    try:
        wfile = os.path.join(ROOT, 'ssq_strategy_weights.json')
        if os.path.exists(wfile):
            with open(wfile, 'r', encoding='utf-8') as wf:
                wj = json.load(wf)
            t_sum = {
                'generated_utc': ts_utc,
                'generated_local': ts_local,
                'window': wj.get('window'),
                'ema_alpha': wj.get('ema_alpha'),
                'weights': wj.get('weights'),
                'metrics': wj.get('metrics', {}),
            }
            with open(os.path.join(ROOT, 'static', 'tuning_summary.json'), 'w', encoding='utf-8') as tf:
                json.dump(t_sum, tf, ensure_ascii=False, indent=2)
    except Exception:
        pass

def main():
    # æ–‡ä»¶åä»ä»¥ç³»ç»Ÿæœ¬åœ°æ—¶é—´ä¸ºå‡†ï¼ˆé€šå¸¸ä¸º UTC ç¯å¢ƒï¼‰ï¼Œä¸æ”¹å˜ç°æœ‰è·¯å¾„è§„èŒƒ
    today = datetime.datetime.now().strftime("%Y%m%d")
    md = os.path.join(REPORTS, f"ssq_status_{today}.md")
    status_text = get_status()
    logs = "\n\n".join([
        tail_log("xuanji_autonomous"),
        tail_log("xuanji_predict"),
        tail_log("xuanji_optimize"),
        tail_log("xuanji_person"),
        tail_log("xuanji_api"),
    ])
    metrics = load_metrics()
    person_metrics = load_person_metrics()
    write_report(md, status_text, logs, metrics, person_metrics)

if __name__ == "__main__":
    main()
